# AI Voice Survey Platform - Product Requirements Document

# CONTENTS

- Approvals
- Abstract
- Business Objectives
- KPI
- Success Criteria
- User Journeys
- Scenarios
- User Flow
- Functional Requirements
- Model Requirements
- Data Requirements
- Prompt Requirements
- Testing & Measurement
- Risks & Mitigations
- Costs
- Assumptions & Dependencies
- Compliance/Privacy/Legal
- GTM/Rollout Plan

## üëç Approvals

| ROLE        | TEAMMATE | REVIEWED | STATUS      |
| ----------- | -------- | -------- | ----------- |
| Product     | TBD      | TBD      | Pending     |
| Engineering | TBD      | TBD      | Pending     |
| UX          | TBD      | TBD      | Not Required (Backend Only) |
| Legal       | TBD      | TBD      | Pending     |

## üìÑ Abstract

AI Voice Survey Platform transforms traditional online surveys into conversational voice experiences. The system ingests Google Forms or Microsoft Forms via shareable links, converts them into structured JSON questionnaires, and deploys an AI voice agent to conduct phone-based surveys. The platform supports two outreach modes: outbound calling to uploaded contact lists and inbound callback requests via unique links. Responses are captured in real-time, mapped to the original survey structure with high accuracy, and written directly to Google Sheets or Microsoft Excel spreadsheets. This eliminates survey fatigue, increases response rates, reduces bias, and delivers richer qualitative data compared to traditional typed surveys.

**Target Users:** Market researchers, political pollsters, news agencies, academic researchers (thesis/PhD students), and companies conducting customer feedback at scale.

**Purpose:** Enable large-scale data collection through low-friction voice interactions, delivering 3-5x more responses with higher quality and less bias than traditional surveys.

## üéØ Business Objectives

- **Increase survey response rates** by reducing participant friction through voice-first interactions versus manual form filling
- **Improve data quality** by capturing longer, less biased responses through natural conversation
- **Accelerate research timelines** by enabling rapid survey deployment and automated data collection at scale
- **Lower operational costs** for market researchers by eliminating manual calling and data entry
- **Enable accessible research** for organizations lacking technical resources through simple form link integration

## üìä KPI

| GOAL                              | METRIC                          | QUESTION                                                                 |
| --------------------------------- | ------------------------------- | ------------------------------------------------------------------------ |
| Survey Processing Volume          | Number of surveys processed     | How many surveys are successfully processed in weeks 1-12?               |
| Response Quality                  | Average response length (words) | Are voice responses 2x+ longer than baseline typed survey responses?    |
| Data Accuracy                     | Response mapping accuracy       | Are we achieving 95%+ correct mapping of responses to survey questions? |

## üèÜ Success Criteria

- **50+ surveys processed** within first 12 weeks of internal testing
- **Response length 2x longer** than typed baseline for comparable surveys
- **95%+ mapping accuracy** validated through manual spot-checking of call logs
- **70%+ call completion rate** for outbound campaigns
- **Zero data security incidents** during testing phase
- **Positive user feedback** from at least 3 pilot researchers on ease of use and data quality

## üö∂‚Äç‚ôÄÔ∏è User Journeys

### Journey 1: Academic Researcher Launching Survey
Dr. Sarah, a PhD candidate studying consumer behavior, needs 200 survey responses for her thesis but has limited time and budget. She creates a Google Form with 15 questions, pastes the shareable link into the platform, and uploads a CSV of 200 phone numbers from her participant recruitment database. She configures the voice agent with a friendly tone and launches the campaign. Over 48 hours, the AI agent completes 140 calls, capturing rich conversational data that flows automatically into her Google Sheet. Sarah exports the data for analysis without manually transcribing a single response.

### Journey 2: Market Research Firm Using Callback Links
A polling firm needs quick feedback on a new product concept. They create a Microsoft Form with 8 questions, generate a callback link through the platform, and distribute it via email to their panel members. Participants click the link, enter their phone number, and receive an immediate call. The voice agent conducts natural 3-5 minute conversations. Responses populate the Excel spreadsheet in real-time, allowing the research team to monitor trends as data comes in and make fast decisions.

## üìñ Scenarios

**Scenario 1: Large-Scale Political Poll**
A news agency uploads 5,000 contacts for a pre-election poll. The system queues calls, manages parallel execution via Twilio, handles busy signals and no-answers with automatic retries, and delivers structured data to Google Sheets within 72 hours.

**Scenario 2: Customer Satisfaction Survey**
A SaaS company wants feedback from 500 users. They create a 10-question form, set a casual voice tone, and upload contacts. The voice agent asks follow-up clarifying questions when responses are unclear, achieving 95% mapping confidence. Partial responses from incomplete calls are flagged for manual review.

**Scenario 3: Academic Research with Consent Requirements**
A university researcher uploads terms and conditions text during survey setup. The voice agent reads the consent statement at the beginning of each call and only proceeds if the participant verbally agrees. All interactions are logged for compliance auditing.

## üïπÔ∏è User Flow

### Happy Path - Outbound Campaign

1. User creates Google/Microsoft Form with survey questions
2. User pastes shareable form link into platform API endpoint
3. Backend scrapes form structure using Playwright, converts to JSON questionnaire, stores in Supabase
4. User configures voice agent settings: tone (friendly/professional/casual), custom instructions, max call duration, retry attempts
5. User uploads terms and conditions text
6. User uploads CSV file with phone numbers and participant metadata
7. User triggers campaign start via API
8. System creates Contact records in Supabase, generates unique callback link
9. FastAPI background task iterates through contacts, triggers Twilio API for each call (non-blocking, parallel)
10. For each call:
    - Twilio connects call
    - OpenAI Realtime API conducts conversation using JSON questionnaire
    - Voice agent reads questions, participant responds verbally
    - Function calling captures structured responses in real-time with confidence scores
    - Responses with 80%+ confidence are mapped immediately; lower confidence flagged for review
11. Call completes, Call_Log record created with transcript, raw responses, mapped responses, status
12. System writes mapped responses to specified Google Sheet/Excel via API
13. User views call logs and export data for analysis

### Alternative Flow - Inbound Callback

1. Steps 1-6 same as outbound
2. System generates callback link: `http://localhost:8000/callback/{survey_id}`
3. User distributes link via email/SMS/web
4. Participant opens link, enters phone number, clicks "Call Me"
5. Backend receives POST request, creates Contact record (callback="OptIn")
6. Twilio initiates immediate call to participant
7. Steps 10-12 same as outbound

## üß∞ Functional Requirements

### API Endpoints

| ENDPOINT                                    | METHOD | PURPOSE                                               | REQUEST                                          | RESPONSE                                     |
| ------------------------------------------- | ------ | ----------------------------------------------------- | ------------------------------------------------ | -------------------------------------------- |
| `/surveys`                                  | POST   | Create survey from form link                          | `{form_link, terms_and_conditions}`              | `{survey_id, json_questionnaire, status}`    |
| `/surveys/{survey_id}`                      | GET    | Retrieve survey details                               | None                                             | Survey object with questionnaire             |
| `/surveys/{survey_id}`                      | PUT    | Update survey details, questionnaire, voice config    | `{form_link?, voice_agent_tone?, instructions?}` | Updated survey object                        |
| `/surveys/{survey_id}/voice-config`         | PUT    | Update voice agent configuration                      | `{tone, instructions, max_duration, retries}`    | Confirmation                                 |
| `/surveys/{survey_id}/activate`             | POST   | Change status to "accepting responses"                | None                                             | `{status: "accepting responses"}`            |
| `/surveys/{survey_id}/contacts/upload`      | POST   | Upload contact list CSV (replaces existing)           | CSV file with phone numbers, names, metadata     | `{contacts_count, upload_timestamp}`         |
| `/surveys/{survey_id}/contacts`             | GET    | List all contacts for survey                          | None                                             | Array of Contact objects                     |
| `/surveys/{survey_id}/calls/start`          | POST   | Trigger outbound calling campaign                     | None                                             | `{job_id, status: "queued"}`                 |
| `/callback/{survey_id}`                     | POST   | Handle inbound callback request                       | `{phone_number, participant_name?}`              | `{contact_id, call_initiated: true}`         |
| `/surveys/{survey_id}/call-logs`            | GET    | Retrieve call logs for survey                         | Query params: status filter                      | Array of Call_Log objects                    |
| `/webhooks/twilio/call-status`              | POST   | Receive call status updates from Twilio               | Twilio request payload                           | 200 OK                                       |
| `/webhooks/twilio/recording`                | POST   | Receive recording availability notification           | Twilio request payload                           | 200 OK                                       |

### Core Features

**Form Ingestion**
- Accept Google Forms and Microsoft Forms shareable links
- Use Playwright to scrape form HTML structure
- Extract question text, question type (multiple choice, checkbox, text, linear scale, dropdown), options, required/optional flags
- Convert to standardized JSON questionnaire format
- Store in `Surveys.json_questionnaire` (JSONB)
- Handle scraping failures gracefully: notify user to check form permissions or contact support

**Voice Agent Configuration**
- Support three predefined tones: friendly, professional, casual
- Accept custom behavior instructions (free text)
- User-configurable max call duration (default: 5 minutes, stored in `Surveys.max_call_duration`)
- User-configurable retry attempts (default: 2, stored in `Surveys.max_retry_attempts`)
- Link to reusable Voice_Agent entity containing model config and tools

**Contact Management**
- Support CSV upload with columns: phone_number (required), participant_name, participant_email, custom metadata
- On new upload, delete all existing contacts for that survey_id before inserting new records
- Track contact source: "uploaded" vs "OptIn" (from callback link)
- Store upload filename and timestamp for auditing

**Outbound Calling**
- Use FastAPI BackgroundTasks to iterate through contact list asynchronously
- For each contact, call Twilio API `client.calls.create()` which returns immediately (non-blocking)
- Twilio manages parallel call execution on their infrastructure
- Respect user-defined max retry attempts for failed calls
- Update Contact.call_status as calls progress (pending ‚Üí completed/failed)
- Create Call_Log entry for each call attempt

**Inbound Callback**
- Generate unique callback link per survey: `http://localhost:8000/callback/{survey_id}`
- Present simple web form: phone number input + "Call Me" button
- On submission, create Contact record with callback="OptIn"
- Trigger immediate Twilio call to provided number
- Use same voice agent conversation flow as outbound

**Real-Time Response Capture**
- Use OpenAI Realtime API function calling to extract structured answers during conversation
- For each question, define function schema matching question type
- Capture confidence score for each response (0-100%)
- Responses with 80%+ confidence: automatically map to `Call_Logs.mapped_responses`
- Responses below 80%: store in `Call_Logs.raw_responses`, flag for manual review
- Handle question types:
  - **Multiple choice:** Read all options, accept verbal answer, use fuzzy matching (e.g., "B", "second one", "the middle option")
  - **Checkboxes:** Announce "you can choose multiple", accept list of options
  - **Linear scale:** Explain endpoints ("1 is lowest, 5 is highest"), accept number
  - **Short answer/Paragraph:** Transcribe verbatim
- Allow participants to skip questions ‚Üí record as "N/A" in spreadsheet
- Fuzzy matching applies only to current question being answered, not skipped questions

**Mid-Survey Handling**
- If participant hangs up mid-survey, store partial responses
- Map responses with 80%+ confidence, leave others as raw
- Set Call_Log.status = "incomplete"
- Contact remains eligible for retry if retry count < max_retry_attempts

**Spreadsheet Integration**
- Write mapped responses to Google Sheets or Microsoft Excel via official APIs
- Use credentials stored in Spreadsheet_Destinations.api_credentials (encrypted)
- Match questionnaire fields to spreadsheet columns
- Append new row for each completed/incomplete call
- Skip field shows "N/A", partial responses show available data

**Logging & Debugging**
- Log all API requests/responses to console
- Log form scraping attempts and outcomes
- Log each call initiation, completion, failure
- Log response mapping process with confidence scores
- Store raw transcripts in Call_Logs.raw_transcript for debugging
- No automated alerts; logs available for manual review

## üîß Model Requirements

| SPECIFICATION          | REQUIREMENT                         | RATIONALE                                                                                                   |
| ---------------------- | ----------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| Open vs Proprietary    | Proprietary (OpenAI Realtime API)   | Native voice-to-voice with low latency, built-in interruption handling, and function calling for structured data capture |
| Context Window         | 128K tokens                         | Sufficient for multi-question surveys with conversation history and system instructions                     |
| Modalities             | Audio (voice input/output)          | Enables natural conversational survey experience without separate STT/TTS pipeline                          |
| Fine Tuning Capability | Not needed for v1                   | Prompt engineering and function calling sufficient for survey question handling                             |
| Latency                | P50: 300ms, P95: 800ms              | Maintains natural conversation flow; delays beyond 1s degrade user experience                               |
| Function Calling       | Required                            | Structured response extraction in real-time with confidence scoring for accurate mapping                    |

## üßÆ Data Requirements

**No fine-tuning required for v1.** The platform relies on prompt engineering and structured function calling to guide the voice agent.

**Data Storage:**
- Store all raw transcripts in `Call_Logs.raw_transcript` for quality assurance and debugging
- Store raw responses (JSON) and mapped responses (JSON) separately in Call_Logs
- Retain call recordings via Twilio recording URLs for compliance and dispute resolution
- Store JSON questionnaires in Supabase for reuse and avoiding repeated form scraping

**Data Preparation:**
- Form scraping output (HTML ‚Üí JSON questionnaire) validated against test forms before production use
- Test response mapping on 10+ sample conversations to validate 95%+ accuracy target

**Data Retention:**
- Call logs retained indefinitely during testing phase (production retention policy TBD)
- Recordings stored per Twilio retention settings (default: indefinitely until deleted)
- Contact data retained until user deletes survey or uploads new contact list

## üí¨ Prompt Requirements

**System Prompt Structure:**

The OpenAI Realtime API system prompt will include:

1. **Role Definition**
   - "You are a professional survey interviewer conducting research on behalf of [User's Organization]."
   - Set tone based on user selection: friendly ("warm and conversational"), professional ("clear and respectful"), casual ("relaxed and approachable")

2. **Task Instructions**
   - Read questions exactly as written in the JSON questionnaire
   - For multiple choice: read all options before asking participant to choose
   - For checkboxes: inform participant they can select multiple options
   - For linear scales: explain the scale endpoints (e.g., "1 means strongly disagree, 5 means strongly agree")
   - Offer to repeat the question if participant requests
   - Use fuzzy matching to interpret responses (e.g., "B", "the second one", "middle option" all map to choice B for current question)
   - If response is unclear, ask for clarification once before accepting and flagging low confidence

3. **Consent Handling**
   - At call start, read user-provided terms and conditions verbatim
   - Ask "Do you consent to participate in this survey?"
   - Only proceed if participant gives clear verbal affirmative (yes, sure, okay, I consent)
   - If declined, thank participant and end call gracefully

4. **Personalization**
   - Use participant name if available in Contact record
   - Apply user-provided custom behavior instructions
   - Maintain consistent tone throughout conversation

5. **Output Format**
   - Use function calling to return structured JSON after each answer
   - Include fields: question_id, question_text, raw_answer, structured_answer, confidence_score (0-100)
   - For skipped questions, return: structured_answer = "N/A", confidence_score = 100

6. **Accuracy & Policy**
   - Do not invent or suggest answers; only capture what participant says
   - Handle profanity or inappropriate content neutrally; transcribe accurately
   - If participant becomes hostile or abusive, politely end call
   - Do not provide survey results, analysis, or commentary
   - Maximum call duration enforced by Twilio timeout parameter

7. **Error Handling**
   - If technical issues occur (connection drops, audio quality issues), attempt to reconnect once
   - If unable to continue, apologize and end call, log as "failed"

## üß™ Testing & Measurement

**Offline Evaluation Plan:**
- **Golden dataset:** 10 test Google/Microsoft Forms with diverse question types (MCQ, checkboxes, linear scale, text)
- **Manual test calls:** Conduct 20+ test calls with scripted responses to validate response mapping
- **Rubric:** Response mapping accuracy = (correctly mapped fields / total fields) √ó 100
- **Pass threshold:** 95%+ mapping accuracy across golden dataset
- **Validation:** Spot-check 10% of call logs manually against recordings to verify transcript and mapping quality

**Online Plan:**
- No A/B testing for v1 (internal testing only)
- Monitor KPIs in real-time via database queries
- **Guardrails:** Max call duration enforced via Twilio timeout; max retry attempts enforced in application logic
- **Rollback:** If mapping accuracy drops below 90% in production, pause campaigns and investigate prompt/function calling issues

**Live Performance Tracking:**
- Track daily: surveys created, calls initiated, calls completed, average call duration, mapping accuracy
- Weekly reporting: KPI dashboard showing survey volume, response length comparison, mapping accuracy trends
- **Alerting:** None for v1; manual log review for debugging

## ‚ö†Ô∏è Risks & Mitigations

| RISK                                                  | LIKELIHOOD | IMPACT | MITIGATION                                                                                                                          |
| ----------------------------------------------------- | ---------- | ------ | ----------------------------------------------------------------------------------------------------------------------------------- |
| Voice agent maps responses incorrectly (< 95%)        | Medium     | High   | Use real-time function calling with confidence scoring; flag low-confidence responses for manual review; test extensively on golden dataset |
| Form scraping breaks when Google/Microsoft changes HTML | Medium     | High   | Use Playwright for resilient scraping; notify user on failure to check form permissions; consider fallback to manual questionnaire upload in v2 |
| High call failure rate (busy, no answer, network)     | High       | Medium | Implement user-configurable retry logic (default 2 attempts); track failure reasons in Call_Logs for debugging                     |
| Twilio/OpenAI API rate limits or downtime             | Low        | High   | Handle API errors gracefully with exponential backoff; log errors for debugging; inform user of service disruptions                 |
| Poor audio quality leads to inaccurate transcription  | Medium     | Medium | Use OpenAI Realtime's built-in noise handling; prompt agent to ask for clarification if audio unclear; store recordings for manual review |
| Data privacy breach or unauthorized access            | Low        | High   | Store API credentials encrypted in Supabase; use environment variables for sensitive keys; limit access to internal testing only   |
| Participant does not consent or requests data deletion | Medium     | Medium | Read T&Cs at call start, only proceed with consent; log consent status; provide mechanism to delete call data upon request (compliance requirement) |
| Cost overruns from long calls or failed retries       | Medium     | Medium | Enforce user-configurable max call duration; limit retries; monitor costs per survey and alert if exceeds expected range            |

## üí∞ Costs

**Development Costs:**
- **Data & QA:** $0 (using test forms and internal contacts)
- **Developer time:** TBD (backend development, integration testing)
- **Third-party services setup:** $0 (free tiers for testing)

**Operational Costs (Per Survey):**

*Assumptions: 100 contacts, 5-minute average call duration, 70% completion rate*

| COST ITEM                    | UNIT COST                | CALCULATION                  | TOTAL         |
| ---------------------------- | ------------------------ | ---------------------------- | ------------- |
| Twilio voice minutes         | ~$0.013/min (US)         | 70 calls √ó 5 min √ó $0.013    | ~$4.55        |
| OpenAI Realtime API          | ~$0.06-0.24/min          | 70 calls √ó 5 min √ó $0.15 avg | ~$52.50       |
| Google Sheets API calls      | Free (quota: 300/min)    | 70 writes                    | $0            |
| Supabase database storage    | Free tier (500MB)        | Negligible for call logs     | $0            |
| **Total per 100-contact survey** |                      |                              | **~$57.05**   |

**Scaling Estimate:**
- 50 surveys in 12 weeks @ 100 contacts each = ~$2,850 total operational cost
- Cost per response: ~$0.57 (competitive with manual calling at $5-10/response)

**Cost Optimization Opportunities (v2):**
- Batch API calls to reduce latency costs
- Use shorter prompts to reduce token usage
- Negotiate volume pricing with Twilio for production

## üîó Assumptions & Dependencies

**Assumptions:**
1. **Frontend development** will occur separately; this PRD covers backend API only
2. **Users have permission** to scrape Google/Microsoft Forms they create (public or organization-accessible)
3. **Form structure is stable** during campaign; changes to form after questionnaire generation may cause mismatches
4. **Participants have working phone numbers** and answer calls from unknown numbers at reasonable rates (target: 70%)
5. **API credentials** for Google Sheets/Excel will be provided by user and stored securely in environment variables
6. **OpenAI Realtime API** remains available and pricing stable during testing phase
7. **Twilio handles parallel calling** efficiently without rate limiting for test volumes (< 100 concurrent calls)
8. **User is legally responsible** for obtaining participant consent and complying with TCPA, GDPR, and local telemarketing laws
9. **Callback link testing** uses localhost; production will require public domain and SSL certificate
10. **Response length baseline** for comparison (typed surveys) will be measured from historical data or parallel typed surveys

**External Dependencies:**
- **OpenAI Realtime API:** Voice conversation, function calling, transcription
- **Twilio API:** Telephony, call management, recording storage
- **Google Sheets API / Microsoft Graph API:** Writing mapped responses to spreadsheets
- **Playwright:** Scraping Google/Microsoft Forms HTML structure
- **Supabase:** Database hosting, encrypted credential storage
- **Python libraries:** FastAPI, Twilio SDK, OpenAI SDK, Playwright, httpx, pandas (for CSV processing)

**Critical Path Dependencies:**
1. OpenAI Realtime API access (waitlist or API key required)
2. Twilio account setup with phone number provisioning
3. Google Cloud Platform / Microsoft Azure app registration for Sheets/Excel API access
4. Supabase project creation and schema deployment

## üîí Compliance/Privacy/Legal

**Regulatory Compliance:**
- **TCPA (Telephone Consumer Protection Act):** User must obtain prior express consent before calling participants; platform requires user to upload terms and conditions and confirm consent is obtained
- **GDPR (General Data Protection Regulation):** If calling EU participants, user must provide privacy notice, obtain consent, and support data deletion requests; platform logs consent at call start
- **CCPA (California Consumer Privacy Act):** Similar to GDPR for California residents; user responsible for compliance
- **Recording Consent Laws:** Some states/countries require two-party consent for call recordings; platform allows user to include recording disclosure in T&Cs

**Data Governance:**
- **PII Handling:** Phone numbers, names, emails, and voice recordings are PII; stored in Supabase with access restricted to authorized users
- **Encryption:** API credentials encrypted at rest in Supabase; TLS for all API communications
- **Retention Policy:** Call logs and recordings retained indefinitely during testing; production policy to be defined (recommend 90 days unless required for compliance)
- **Access Controls:** No authentication for v1 (internal testing); production will require API key or JWT authentication
- **Data Deletion:** Provide endpoint for users to delete survey data including call logs and recordings upon request (GDPR Article 17 Right to Erasure)

**Terms of Service (User Responsibility):**
- User warrants they have legal right to call all uploaded contacts
- User warrants they have obtained necessary consent from participants
- User is responsible for providing accurate terms and conditions
- User agrees to comply with all applicable telemarketing and data privacy laws
- Platform disclaims liability for user's misuse or non-compliance

**Audit Trail:**
- Log all consent interactions (participant says yes/no to T&Cs)
- Store upload timestamps for contact lists
- Retain raw transcripts for dispute resolution
- Track survey creator (user_id) for accountability

## üì£ GTM/Rollout Plan

**Phase 1: Internal Development & Testing (Weeks 1-4)**
- Set up development environment (Supabase, Twilio sandbox, OpenAI API)
- Implement core backend: form scraping, JSON questionnaire generation, database schema
- Build API endpoints for survey creation and contact upload
- Test form scraping on 10 diverse Google/Microsoft Forms
- **Exit Criteria:** Successfully scrape and convert 10 forms to JSON with 100% accuracy

**Phase 2: Voice Agent Integration (Weeks 5-8)**
- Integrate OpenAI Realtime API with Twilio
- Implement function calling for structured response capture
- Build real-time response mapping logic with confidence scoring
- Conduct 20+ manual test calls to validate conversation flow and mapping accuracy
- **Exit Criteria:** Achieve 95%+ mapping accuracy on golden dataset

**Phase 3: Spreadsheet Integration & End-to-End Testing (Weeks 9-10)**
- Integrate Google Sheets and Microsoft Excel APIs
- Implement automatic response writing after call completion
- Test full workflow: form ingestion ‚Üí voice calls ‚Üí spreadsheet output
- Validate callback link flow (inbound calls)
- **Exit Criteria:** Complete 5 end-to-end test surveys with successful data export

**Phase 4: Pilot with Internal Users (Weeks 11-12)**
- Onboard 3 internal researchers to test platform with real surveys
- Collect qualitative feedback on usability, data quality, and pain points
- Monitor KPIs: surveys processed, response length, mapping accuracy
- Identify bugs and edge cases
- **Exit Criteria:** Process 50+ surveys, achieve KPI targets, positive user feedback

**Phase 5: Iteration & Production Readiness (Weeks 13-16)**
- Fix bugs identified in pilot
- Improve form scraping resilience based on failure patterns
- Optimize prompts to improve mapping accuracy and conversation naturalness
- Prepare for production rollout: authentication, public callback domain, monitoring setup
- **Exit Criteria:** Platform stable, ready for limited production beta

**Launch Strategy:**
- **Private beta:** Invite 10-20 external researchers from academic and market research communities
- **Feedback loop:** Weekly check-ins with beta users, prioritize feature requests
- **Gradual scale:** Start with 100 surveys/month capacity, scale infrastructure based on demand

**Phased Rollout:**
- **Week 1-2:** Internal testing only (5 surveys)
- **Week 3-6:** Limited pilot (3 users, 20 surveys)
- **Week 7-12:** Expanded pilot (10 users, 50+ surveys)
- **Week 13+:** Iterate based on feedback, prepare production launch

**Success Metrics for Each Phase:**
- Phase 1-3: Technical milestones (form scraping accuracy, mapping accuracy, API stability)
- Phase 4: User adoption (50+ surveys processed) + data quality (95%+ mapping, 2x response length)
- Phase 5: Platform stability (< 5% error rate) + user satisfaction (NPS > 40)

---

**Document Version:** 1.0  
**Last Updated:** October 8, 2025  
**Prepared By:** Product Manager (AI Voice Survey Team)  
**Next Review Date:** TBD (post-Phase 4 pilot completion)